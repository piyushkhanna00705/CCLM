NNODES,  1
NPROC_PER_NODE,  8
MASTER_ADDR,  SET_IT
MASTER_PORT,  12345
NODE_RANK,  0
### warning: the settings for distributed training is not filled (ignore this if you only use one node)
### warning: you have not set the path to hadoop_bin (ignore this if you don't use HDFS)
### Training VQA
| distributed init (rank 0): env://
Creating vqa datasets
### data 10000, batch size, 128 x 1
### Test: [('bn', 9666), ('de', 9666), ('en', 9666), ('id', 9666), ('ko', 9666), ('pt', 9666), ('ru', 9666), ('zh', 9666)]
### be careful: func create_loader returns a list length of 1
### be careful: func create_loader returns a list length of 1
### be careful: func create_loader returns a list length of 1
### be careful: func create_loader returns a list length of 1
### be careful: func create_loader returns a list length of 1
### be careful: func create_loader returns a list length of 1
### be careful: func create_loader returns a list length of 1
### be careful: func create_loader returns a list length of 1
Creating model
### pad_token_id,  1
### eos_token,  </s>
### Loading pretrained vision encoder
### Loading pretrained text encoder
load checkpoint from cclm_3m_epoch_29.th
missing_keys:  []
unexpected_keys:  ['temp', 'text_proj.weight', 'text_proj.bias', 'vision_proj.weight', 'vision_proj.bias', 'itm_head.0.weight', 'itm_head.0.bias', 'itm_head.1.weight', 'itm_head.1.bias', 'itm_head.3.weight', 'itm_head.3.bias', 'bbox_head.0.weight', 'bbox_head.0.bias', 'bbox_head.1.weight', 'bbox_head.1.bias', 'bbox_head.3.weight', 'bbox_head.3.bias']
### Total Params:  878245194
### output_dir,  CCLM_output3_10k_no_captions/
### output_hdfs,  
Start training
### lr_mult,  2
### model has 'init_params',  0
### num_training_steps,  395
### num_warmup_steps,  39
Train Epoch: [0] [ 0/78]  eta: 0:05:22  lr: 0.000001  loss: 28.8813  time: 4.1368  data: 1.2866  max mem: 43082
